{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oPVSwgevVJ1C"
   },
   "source": [
    "## Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "_jnJ6EhDVXQ6"
   },
   "outputs": [],
   "source": [
    "import gzip\n",
    "import numpy as np\n",
    "import random as rd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "from keras.layers import LSTM, GRU, Dense, RepeatVector, TimeDistributed, Input, \\\n",
    "    multiply, concatenate, Flatten, Activation, dot, Bidirectional, Embedding, Dropout, Conv1D, \\\n",
    "    MultiHeadAttention, Add, LayerNormalization, BatchNormalization\n",
    "\n",
    "from keras.models import Sequential, Model, load_model\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from keras.utils.vis_utils import plot_model, model_to_dot\n",
    "\n",
    "from tensorflow.keras.regularizers import L1, L2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5534, 700)\n"
     ]
    }
   ],
   "source": [
    "f = gzip.GzipFile('../Data/Preprocessed/CullPDB_6133_filtered_profile_MASK.npy.gz', \"r\")\n",
    "CullPDB_profile = np.load(f)\n",
    "f.close()\n",
    "\n",
    "f = gzip.GzipFile('../Data/Preprocessed/CullPDB_6133_filtered_sequence_MASK.npy.gz', \"r\")\n",
    "CullPDB_sequence = np.load(f)\n",
    "f.close()\n",
    "\n",
    "## CullPDB_sequence_integer : transform one hot encoding into a single integer for Embedding input\n",
    "CullPDB_sequence_integer = np.zeros((CullPDB_sequence.shape[0], CullPDB_sequence.shape[1]))\n",
    "print(CullPDB_sequence_integer.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fUqcJ9VTVvK6"
   },
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "79r1mItlFQ3u",
    "outputId": "1900b98d-a6ef-4ef6-96c0-1a5c5aae1d43"
   },
   "outputs": [],
   "source": [
    "\n",
    "def prepareData ():\n",
    "    f = gzip.GzipFile('../Data/Preprocessed/CullPDB_6133_filtered_profile_MASK.npy.gz', \"r\")\n",
    "    CullPDB_profile = np.load(f)\n",
    "    f.close()\n",
    "\n",
    "    f = gzip.GzipFile('../Data/Preprocessed/CullPDB_6133_filtered_sequence_MASK.npy.gz', \"r\")\n",
    "    CullPDB_sequence = np.load(f)\n",
    "    f.close()\n",
    "\n",
    "    ## CullPDB_sequence_integer : transform one hot encoding into a single integer for Embedding input\n",
    "    CullPDB_sequence_integer = np.zeros((CullPDB_sequence.shape[0], CullPDB_sequence.shape[1]))\n",
    "    print(CullPDB_sequence_integer.shape)\n",
    "    for i in range(0, CullPDB_sequence_integer.shape[0]):\n",
    "        for j in range(0, CullPDB_sequence_integer.shape[1]):\n",
    "            CullPDB_sequence_integer[i][j] = np.argmax(CullPDB_sequence[i][j])\n",
    "    print(CullPDB_sequence_integer)\n",
    "\n",
    "    f = gzip.GzipFile(\"../Data/Preprocessed/CullPDB_6133_filtered_traininglabel_MASK.npy.gz\", \"r\")\n",
    "    CullPDB_traininglabel = np.load(f)\n",
    "    f.close()\n",
    "\n",
    "    hmm_profile = np.load(\"Data/hmm_train.npy\", \"r\")\n",
    "\n",
    "    CullPDB_data = {\"sequence\": CullPDB_sequence_integer, \"profile\": CullPDB_profile, \"label\":CullPDB_traininglabel, \"hmm_profile\":hmm_profile}\n",
    "\n",
    "    return CullPDB_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1RnLJIwUVTng"
   },
   "source": [
    "## Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "_0t9MtvuVWYf"
   },
   "outputs": [],
   "source": [
    " def buildModel (data, gru_hidden, do, epoch, bs) :\n",
    "\n",
    "    sequence_length= 700\n",
    "    input_dim = 63\n",
    "    output_dim = 9\n",
    "    pssm_input_dim = 21\n",
    "    hmm_input_dim = 20\n",
    "    embedding_input_dim = 22\n",
    "    embedding_output_dim = 22\n",
    "    units = 256\n",
    "\n",
    "\n",
    "    strategy = tf.distribute.MirroredStrategy()\n",
    "    with strategy.scope():\n",
    "\n",
    "        ## Layers\n",
    "        # input: pssm and one hot encoding\n",
    "        pssm_input = Input(shape=(sequence_length, pssm_input_dim))\n",
    "        embedding_input=Input(shape=(sequence_length, ))\n",
    "        print(embedding_input.shape)\n",
    "        print(pssm_input.shape)\n",
    "\n",
    "        hmm_input = Input(shape=(sequence_length, hmm_input_dim))\n",
    "        print(hmm_input.shape)\n",
    "\n",
    "        output = Input(shape=(sequence_length, output_dim))\n",
    "        print(output.shape)\n",
    "\n",
    "        # embedding layer for the one hot encoding\n",
    "        embedding_out= Embedding(input_dim=embedding_input_dim, output_dim=embedding_output_dim, input_length=sequence_length, mask_zero=True)(embedding_input)\n",
    "        print(embedding_out.shape)\n",
    "        model_input =  tf.concat([embedding_out, pssm_input, hmm_input], axis=2)\n",
    "        \n",
    "        print(model_input.shape)\n",
    "\n",
    "        ## Self attention here\n",
    "        ## No Mask is used here\n",
    "        \n",
    "        att_output, att_scores= MultiHeadAttention(num_heads=1, key_dim=model_input.shape[-1], dropout=0.5)(query=model_input, value=model_input, return_attention_scores=True)\n",
    "        model_input_att = Add()([model_input, att_output])\n",
    "        model_input_att = LayerNormalization()(model_input_att)\n",
    "        model_input_att = BatchNormalization(synchronized=True)(model_input_att)\n",
    "\n",
    "        ## Block 1\n",
    "\n",
    "        model_input1 = Dense(63, activation='relu')(model_input_att)\n",
    "        model_input2 = Dense(128, activation='relu')(model_input1)\n",
    "\n",
    "        whole_sequence_output1, final_state_1f, final_state_1b = Bidirectional(GRU(128, return_state=True, return_sequences=True, dropout=0.3))(model_input2)\n",
    "        whole_sequence_output2, final_state_2f, final_state_2b = Bidirectional(GRU(128, return_state=True, return_sequences=True, dropout=0.3))(whole_sequence_output1)\n",
    "\n",
    "        whole_sequence_output2=BatchNormalization(synchronized=True)(whole_sequence_output2)\n",
    "\n",
    "        ## Block 2\n",
    "\n",
    "        model_input3 = Dense(63, activation='relu')(model_input_att)\n",
    "        model_input4 = Dense(256, activation='relu')(model_input3)\n",
    "\n",
    "        whole_sequence_output3, final_state_3f, final_state_3b = Bidirectional(GRU(256, return_state=True, return_sequences=True, dropout=0.5))(model_input4)\n",
    "        whole_sequence_output4, final_state_4f, final_state_4b = Bidirectional(GRU(256, return_state=True, return_sequences=True, dropout=0.5))(whole_sequence_output3)\n",
    "\n",
    "        whole_sequence_output4=BatchNormalization(synchronized=True)(whole_sequence_output4)\n",
    "\n",
    "        ## Block 3\n",
    "        model_input5 = Dense(63, activation='relu')(model_input_att)\n",
    "        model_input6 = Dense(512, activation='relu')(model_input5)\n",
    "\n",
    "        whole_sequence_output5, final_state_5f, final_state_5b = Bidirectional(GRU(512, return_state=True, return_sequences=True, dropout=0.6))(model_input6)\n",
    "        whole_sequence_output6, final_state_6f, final_state_6b = Bidirectional(GRU(512, return_state=True, return_sequences=True, dropout=0.6))(whole_sequence_output5)\n",
    "\n",
    "        whole_sequence_output6=BatchNormalization(synchronized=True)(whole_sequence_output6)\n",
    "\n",
    "        ## Combine the blocks\n",
    "\n",
    "        whole_sequence_output = tf.concat([whole_sequence_output2, whole_sequence_output4, whole_sequence_output6], axis=-1)\n",
    "        whole_sequence_output = Dropout(do)(whole_sequence_output)\n",
    "        \n",
    "      \n",
    "        # Fully Connected Layers\n",
    "\n",
    "        dense0= Dense(512, activation='relu')(whole_sequence_output)\n",
    "        dense1= Dense(256, activation='relu')(dense0)\n",
    "        dense2= Dense(128, activation='relu')(dense1)\n",
    "        \n",
    "        out = TimeDistributed(Dense(output.shape[2], activation='softmax'))(dense2)\n",
    "\n",
    "\n",
    "        print(out.shape)\n",
    "\n",
    "        ## MODEL TRAINING\n",
    "        model = Model(inputs=[embedding_input, pssm_input, hmm_input], outputs=out)\n",
    "        opt = Adam(learning_rate=0.0001)\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "    model.summary()\n",
    "\n",
    "    plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)\n",
    "\n",
    "    epc = epoch\n",
    "    es = EarlyStopping(monitor='val_loss', mode='min', patience=10)\n",
    "    history = model.fit([data[\"sequence\"], data[\"profile\"], data[\"hmm_profile\"]], data[\"label\"],\n",
    "                    epochs=epc, verbose=1, callbacks=[es], validation_split=0.1,\n",
    "                    batch_size=bs)\n",
    "\n",
    "    #Plot the training loss\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "\n",
    "\n",
    "    # Save the model\n",
    "    model_name = 'model'\n",
    "    model.save(model_name+'.h5')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "JYaOIPjkh9sM",
    "outputId": "a71d6c6b-c957-4342-e9b1-05e3935a220a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5534, 700)\n",
      "[[ 6.  4. 20. ...  0.  0.  0.]\n",
      " [16. 14.  3. ...  0.  0.  0.]\n",
      " [10.  9.  7. ...  0.  0.  0.]\n",
      " ...\n",
      " [15.  7. 12. ...  0.  0.  0.]\n",
      " [ 1.  6.  1. ...  0.  0.  0.]\n",
      " [10.  9. 16. ...  0.  0.  0.]]\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Data/hhm_train.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Call prepareData function\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m train_data\u001b[38;5;241m=\u001b[39m \u001b[43mprepareData\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Call buildModel function\u001b[39;00m\n\u001b[0;32m      5\u001b[0m buildModel(train_data, \u001b[38;5;241m256\u001b[39m, \u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m64\u001b[39m)\n",
      "Cell \u001b[1;32mIn[2], line 22\u001b[0m, in \u001b[0;36mprepareData\u001b[1;34m()\u001b[0m\n\u001b[0;32m     19\u001b[0m CullPDB_traininglabel \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[0;32m     20\u001b[0m f\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m---> 22\u001b[0m hmm_profile \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mData/hhm_train.npy\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m CullPDB_data \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msequence\u001b[39m\u001b[38;5;124m\"\u001b[39m: CullPDB_sequence_integer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprofile\u001b[39m\u001b[38;5;124m\"\u001b[39m: CullPDB_profile, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m:CullPDB_traininglabel, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhmm_profile\u001b[39m\u001b[38;5;124m\"\u001b[39m:hmm_profile}\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m CullPDB_data\n",
      "File \u001b[1;32mc:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\numpy\\lib\\npyio.py:405\u001b[0m, in \u001b[0;36mload\u001b[1;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[0;32m    403\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    404\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 405\u001b[0m     fid \u001b[38;5;241m=\u001b[39m stack\u001b[38;5;241m.\u001b[39menter_context(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mos_fspath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    406\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    408\u001b[0m \u001b[38;5;66;03m# Code to distinguish from NumPy binary files and pickles.\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Data/hhm_train.npy'"
     ]
    }
   ],
   "source": [
    "# Call prepareData function\n",
    "train_data= prepareData()\n",
    "\n",
    "# Call buildModel function\n",
    "buildModel(train_data, 256, 0.5, 100, 64)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
